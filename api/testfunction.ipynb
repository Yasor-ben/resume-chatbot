{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START ,END\n",
    "from chain import State, call_model, print_update, should_continue, summarize_history, summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain est un cadre d'orchestration open source qui permet de développer des applications à l'aide de grands modèles de langage (LLM). Il propose un environnement centralisé pour créer des applications LLM et les intégrer à des sources de données externes et à des workflows logiciels. LangChain facilite la plupart des cas d'utilisation des LLM et du traitement automatique du langage naturel, comme les chatbots, la recherche intelligente et les agents conversationnels. Il permet d'importer facilement des modèles de langage, de structurer des prompts efficaces, de créer des chaînes pour exécuter des modèles et de connecter des sources de données externes. LangChain propose également des outils pour surveiller, évaluer et déboguer les applications. Il est open source et peut être installé sur Python. Les cas d'utilisation de LangChain incluent les chatbots, la synthèse de texte, les réponses aux questions, l'augmentation de données et les agents conversationnels.\n"
     ]
    }
   ],
   "source": [
    "with open(\"doclangchain.txt\", \"r\") as fich:\n",
    "    contenu = fich.read()\n",
    "    \n",
    "summer = summarize(contenu)\n",
    "print(summer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"conversation\", lambda input: call_model(state=input, conversation_summary=summer))\n",
    "\n",
    "workflow.add_node(summarize_history)\n",
    "\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue,)\n",
    "\n",
    "workflow.add_edge(\"summarize_history\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "qu'elle est l'information importante de ce texte ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "L'information importante de ce texte est que LangChain est un cadre d'orchestration open source qui permet de développer des applications à l'aide de grands modèles de langage (LLM). Il offre un environnement centralisé pour créer des applications LLM, les intégrer à des sources de données externes et à des workflows logiciels. LangChain facilite divers cas d'utilisation des LLM et du traitement automatique du langage naturel, tels que les chatbots, la recherche intelligente et les agents conversationnels. Il permet d'importer des modèles de langage, de structurer des prompts efficaces, de créer des chaînes pour exécuter des modèles et de connecter des sources de données externes. De plus, LangChain propose des outils pour surveiller, évaluer et déboguer les applications. Enfin, LangChain est open source et peut être installé sur Python.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\":\"4\"}}\n",
    "input_message = HumanMessage(content=\"qu'elle est l'information importante de ce texte ?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "peut on faire un RAG avec ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Je suis désolé, mais je ne dispose pas d'informations spécifiques sur la capacité de LangChain à effectuer un RAG (Résumé Automatique Génératif). Cependant, étant donné que LangChain est conçu pour faciliter divers cas d'utilisation du traitement automatique du langage naturel, il est possible qu'il puisse être utilisé pour développer des fonctionnalités de résumé automatique. Il serait préférable de consulter la documentation officielle de LangChain ou de contacter les développeurs de l'outil pour obtenir des informations plus précises à ce sujet.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"peut on faire un RAG avec ?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
